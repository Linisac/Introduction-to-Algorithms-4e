\setcounter{chapter}{4}
\chapter{Probabilistic Analysis and Randomized Algorithms}
%Section 5.1------------------------------------------------------------------------------------------------------------
\section{The hiring problem}
\subsection*{Exercises}
\begin{enumerate}[\thesection-1]
%
\item Recall the definition of total order. Since the order in which the candidates are presented is random, every pair of candidates can appear as candidate $1$ and candidate $2$. Use the fact that you always know whether candidate $2$ is better than candidate $1$.
%
\item Let $m \defas b - a + 1$. If $m = 2^n$ for some $n \in \pint$, then assign the binary strings over $\sete{0, 1}$ of length $n$ (say, in lexicographic order) to the numbers $a, \etl, b$; make $n$ calls to $\proc{Random}(0, 1)$ in a row. Otherwise, let $n \defas \ceil{\lg m}$, and assign the first $m$ binary strings (in lexicographic order) to the numbers $a, \etl, b$; make $n$ calls to $\proc{Random}(0, 1)$ in a row, and when it produces the remaining $2^n - m$ unassigned binary strings, perform $n$ calls to $\proc{Random}(0, 1)$ again until it produces an assigned binary string.
%
\item An algorithm is given as follows.
\begin{codebox}
\Procname{$\proc{Unbiased-Random}$}
\li $b_1 \gets \proc{Biased-Random}$
\li \If $b_1 \isequal 0$
\li \Then
          $b_2 \gets \proc{Biased-Random}$
\li       \If $b_2 \isequal 0$
\li       \Then
                \Return $\proc{Unbiased-Random}$
\li       \Else
\li             \Return $0$
\zi             \Comment{The case of $(b_1, b_2) = (0, 1)$ occurs with probability $p(1 - p)$ and is assigned to the value $0$}
          \End
\li \Else \Comment{$b_1 \isequal 1$}
\li       $b_2 \gets \proc{Biased-Random}$
\li       \If $b_2 \isequal 0$
\li       \Then
                \Return $1$
\zi             \Comment{The case of $(b_1, b_2) = (1, 0)$ occurs with probability $p(1 - p)$ and is assigned to the value $1$}
\li       \Else
\li             \Return $\proc{Unbiased-Random}$
          \End
     \End
\end{codebox}
%
\end{enumerate}
%End of Section 5.1-----------------------------------------------------------------------------------------------------

\setcounter{section}{2}
%Section 5.3------------------------------------------------------------------------------------------------------------
\section{Randomized algorithms}
\begin{enumerate}[\thesection-1]
%
\item Unroll the \For loop of $\proc{Randomly-Permute}(A, n)$ for $i = 1$.
\begin{codebox}
\Procname{$\proc{Randomly-Permute}'(A, n)$}
\li swap $A[1]$ with $A[\proc{Random}(1, n)]$
\li \For $i \gets 2$ \To $n$
\li \Do
        swap $A[i]$ with $A[\proc{Random}(i, n)]$
    \End
\end{codebox}
The associated loop invariant is below.
\begin{loopinv}
Just prior to the $(i - 1)$st iteration of the \For loop of lines $2$-$3$, for each possible $(i - 1)$-permutation of the $n$ elements, the subarray $A[1 \subarr i - 1]$ contains this $(i - 1)$-permutation with probability $(n - i + 1)!/n!$.
\end{loopinv}
%
\setcounter{enumi}{2}
%
\item Let $n \geq 3$. The procedure $\proc{Permute-With-All}(A, n)$ does not produce a uniform random permutation. To see this, consider a complete $n$-ary rooted tree of height $n$ in which the $n$ children of each node at depth $(i - 1)$, where $i = 1, \etl, n$, correspond to the $n$ possible outcomes produced by the call to $\proc{Random}(1, n)$ in the $i$th iteration of the \For loop of $\proc{Permute-With-All}(A, n)$; thus, the leaves give exactly the possible outcomes by executing $\proc{Permute-With-All}(A, n)$. Note that there are $n^n$ leaves, while the total number of $n$-permutations of $A$ is $n!$. Since $n - 1$ does not divide $n$, neither does $n!$ divide $n^n$. Consequently, some $n$-permutations are more likely than others.
%
\setcounter{enumi}{4}
%
\item Prove the loop invariant below.
\begin{loopinv}
Just prior to the $(k - n + m)$th iteration of the \For loop of lines $2$-$6$, every $(k - n + m - 1)$-combination of the set $\sete{1, \etl, k - 1}$ is equally likely to be the set $S$, with probability $1/{k - 1 \choose k - n + m - 1}$.
\end{loopinv}
In the maintenance part, distinguish two cases based on whether or not it holds that $i \in S$ or $i = k$ immediately after the call to $\proc{Random}(i, k)$ on line $3$ of the procedure $\proc{Random-Sample}(m, n)$.

\begin{remark}
Like many methods based on randomness (e.g., Exercise 5.1-3), the procedure $\proc{Random-Sample}(m, n)$ works by ``interpreting'' the outcomes of executing a random procedure to achieve a desired distribution. A similar idea that kind of goes the other way around is the following counting argument to show that the number of pairs $(a, b)$, where $a \leq b$ are from the set $\sete{1, \etl, n}$, is equal to ${n + 1 \choose 2}$: The number of such pairs is equal to the number of $2$-combinations of the set $\sete{1, \etl, n, \mbox{``duplicate''}}$, where for each $k \in \sete{1, \etl, n}$ the $2$-combination $\sete{i, \mbox{``duplicate''}}$ is interpreted as the pair $(k, k)$.
\end{remark}
%
\end{enumerate}
%End of Section 5.3-----------------------------------------------------------------------------------------------------

%Section 5.4------------------------------------------------------------------------------------------------------------
\section{Probabilistic analysis and further uses of indicator random variables}
\begin{enumerate}[\thesection-1]
\setcounter{enumi}{2}
%
\item Note how this exercise question is similar to the birthday paradox presented in Subsection 5.4.1~in text. For $1 \leq k \leq b + 1$ let $A_k$ be the event that the $k$th toss ends up in an empty bin, and for $1 \leq k \leq b$ let $B_k$ be the event that the first $k$ tosses end up in an empty bin each (these events are similar to those given in 5.4.1). We have $\pr{A_1} = \pr{B_1} = 1$ and $\pr{A_{b + 1}} = 0$; more importantly, for $2 \leq k \leq b$, we have
\[
\pr{A_k \given B_{k - 1}} = \parenf{1 - \frac{k - 1}{b}}
\]
and hence
\[
\begin{array}{lcl}
\pr{B_k} & = & \pr{B_{k - 1}} \pr{A_k \given B_{k - 1}} \cr
         & \etv & \cr
         & = & \pr{B_1} \pr{A_2 \given A_1} \pr{A_3 \given B_2} \etc \pr{A_k \given B_{k - 1}} \cr
         & = & \ds{1 \multp \parenf{1 - \frac{1}{b}} \parenf{1 - \frac{2}{b}} \etc \parenf{1 - \frac{k - 1}{b}}}. \cr
\end{array}
\]
because $B_k = A_k \intsc B_{k - 1}$.

Let $X$ be the number of tosses until some bin contains two balls. Then, for $2 \leq k \leq b + 1$ we have
\[
\pr{X = k} = \pr{\cmpl{A_k} \intsc B_{k - 1}} = \pr{B_{k - 1}} \pr{\cmpl{A_k} \given B_{k - 1}} = 1 \multp \parenf{1 - \frac{1}{b}} \parenf{1 - \frac{2}{b}} \etc \parenf{1 - \frac{k - 2}{b}} \multp \frac{k - 1}{b}.
\]
It suffices to compute
\[
\ex{X} = \sum^{b + 1}_{k = 2} k \pr{X = k}.
\]
%
\item The birthdays are mutually independent. On page 141, the probability that all $k$ people in the room have distinct birthdays is
\[
\pr{B_k} = 1 \multp \parenf{1 - \frac{1}{n}} \parenf{1 - \frac{2}{n}} \etc \parenf{1 - \frac{k - 1}{n}}.
\]
Since there are ${n \choose k} k!$ different cases in which all the $k$ people have distinct birthdays, namely $\card{B_k} = {n \choose k} k!$, and since each case is equally likely due to the uniform distribution of birthdays, we have that, for all distinct numbers $n_1, \etl, n_k$ in the range $1, \etl, n$, the probability
\[
\pr{b_1 = n_1, \etl, b_k = n_k} = \frac{\pr{B_k}}{{n \choose k} k!} = \frac{1}{n^k}.
\]
This means that it is implicitly assumed that the birthdays $b_1, \etl, b_k$ are mutually independent in the analysis.
%
\item Assume that there are $n = 365$ days in a year and that there are $k$ people in a room. Let $E_m$ be the event that among the $k$ people, there are exactly $m$ pairs of people sharing the same birthday, each pair a distinct birthday. Then
\[
\card{E_0} = {n \choose k} \multp k!
\]
and
\[
\card{E_m} = \frac{{k \choose 2}{k - 2 \choose 2} \etc {k - 2m + 2 \choose 2}}{m!} \multp {n \choose k - m} \multp (k - m)!
\]
for $m \geq 1$. Hence,
\[
\pr{E_m} = \frac{\card{E_m}}{n^k}
\]
for $m \geq 0$ since the birthdays are mutually independent. The probability that there are at least three people sharing the same birthday is
\[
1 - \sum^{\floor{\frac{k}{2}}}_{m = 0} \pr{E_m}.
\]
%
\item The probability that a $k$-string over a set of size $n$ forms a $k$-permutation is $\frac{{n \choose k} k!}{n^k}$. This is analogous to the birthday paradox in that there are $n$ days in a year and $k$ people in a room.
%
\item For $1 \leq i \leq n$, let $X_i$ and $Y_i$ be indicator random variables such that
\[
X_i = \ind{\mbox{the \mathmode{i}th bin is empty}},
\]
and
\[
Y_i = \ind{\mbox{the \mathmode{i}th bin contains exactly one ball}}.
\]
then we have
\[
\pr{X_i = 1} = \frac{(n - 1)^n}{n^n} = \parenf{1 - \frac{1}{n}}^n
\]
and
\[
\pr{Y_i = 1} = \frac{{n \choose 1}(n - 1)^{n - 1}}{n^n} = \parenf{1 - \frac{1}{n}}^{n - 1}.
\]

Thus,
\[
\ex{\sum^n_{i = 1} X_i} = \sum^n_{i = 1} \ex{X_i} = n \multp \parenf{1 - \frac{1}{n}}^n
\]
and
\[
\ex{\sum^n_{i = 1} Y_i} = \sum^n_{i = 1} \ex{Y_i} = n \multp \parenf{1 - \frac{1}{n}}^{n - 1}.
\]
%
\item Skipped.
%
\end{enumerate}
%End of Section 5.4-----------------------------------------------------------------------------------------------------

%Problems of Chapter 5--------------------------------------------------------------------------------------------------
\section*{Problems}
\begin{enumerate}[\thechapter-1]
%
\item
\begin{enumerate}[(a)]
%%
\item Let us prove the claim for $0 \leq n \leq 2^b - 1$. First of all, every call to the procedure \proc{Increment} is a Bernoulli trial, where a success increments the counter value by $1$ and a failure leaves the counter unchanged. Denoting a success by $1$ and a failure by $0$, the outcome of performing $n$ \proc{Increment} operations can be represented by an $n$-string over $\sete{0, 1}$, in other words, the associated sample space is $\cartpwr{\sete{0, 1}}{n}$.
\medskip\\
Consider the sample space $\cartpwr{\sete{0, 1}}{n}$ for every $0 \leq n \leq 2^b - 1$. For all $s \in \cartpwr{\sete{0, 1}}{n}$, we write $i(s)$ for the number of $1$s in $s$ and $A_s$ for the elementary event $s$ (i.e., $A_s = \sete{s}$). Moreover, if $n \geq 1$, then we write $B_n$ for the event of all outcomes that end with a success (i.e., the set of all $n$-strings whose last character is a $1$) and, for $(n - 1)$-strings $s$, we write $A'_s$ for the event of all outcomes whose prefix is $s$ (i.e., $A'_s = \sete{s0, s1}$).\footnote{Here $s0$ and $s1$ denote the $n$-strings obtained by appending $0$ and $1$ to $s$, respectively.}
\medskip\\
It follows that for all $1 \leq n \leq 2^b - 1$ and all $(n - 1)$-strings $s$, we have that
\[
\pr{A'_s} = \pr{A_s}
\]
and
\[
\pr{B_n \given A'_s} = \frac{1}{n_{i(s) + 1} - n_{i(s)}}
\]
and that
\[
\pr{A_{s1}} = \pr{A'_s \intsc B_n} = \pr{A'_s} \multp \pr{B_n \given A'_s}
\]
while
\[
\pr{A_{s0}} = \pr{A'_s \intsc \cmpl{B_n}} = \pr{A'_s} \multp \pr{\cmpl{B_n} \given A'_s} = \pr{A'_s} \multp (1 - \pr{B_n \given A'_s}).
\]

Let $X_n$ denote the value represented by the counter after $n$ \proc{Increment} operations have been performed. The claim can then be proved by induction on $0 \leq n \leq 2^b - 1$: The base case is trivial. For the inductive case, let $1 \leq n \leq 2^b - 1$, we have
\[
\begin{array}{lll}
\ex{X_n} & = & \ds{\sum_{s \in \cartpwr{\sete{0, 1}}{n}} n_{i(s)} \multp \pr{A_s}} \cr
         & = & \ds{\sum_{s \in \cartpwr{\sete{0, 1}}{n - 1}} (n_{i(s1)} \multp \pr{A_{s1}} + n_{i(s0)} \multp \pr{A_{s0}})} \cr
         & = & \ds{\sum_{s \in \cartpwr{\sete{0, 1}}{n - 1}} (n_{i(s) + 1} \multp \pr{A_{s1}} + n_{i(s)} \multp \pr{A_{s0}})} \cr
         & = & \ds{\sum_{s \in \cartpwr{\sete{0, 1}}{n - 1}} (n_{i(s) + 1} \multp \pr{A'_s} \multp \pr{B_n \given A'_s} + n_{i(s)} \multp \pr{A'_s} \multp (1 - \pr{B_n \given A'_s}))} \cr
         & = & \ds{\sum_{s \in \cartpwr{\sete{0, 1}}{n - 1}} (n_{i(s)} \multp \pr{A'_s} + (n_{i(s) + 1} - n_{i(s)}) \multp \pr{A'_s} \multp \pr{B_n \given A'_s})} \cr
         & = & \ds{\sum_{s \in \cartpwr{\sete{0, 1}}{n - 1}} (n_{i(s)} \multp \pr{A_s} + (n_{i(s) + 1} - n_{i(s)}) \multp \pr{A_s} \multp \pr{B_n \given A'_s})} \cr
         & = & \ds{\sum_{s \in \cartpwr{\sete{0, 1}}{n - 1}} (n_{i(s)} \multp \pr{A_s} + \pr{A_s})} \cr
         & = & \ds{\sum_{s \in \cartpwr{\sete{0, 1}}{n - 1}} (n_{i(s)} \multp \pr{A_s}) + \sum_{s \in \cartpwr{\sete{0, 1}}{n - 1}} \pr{A_s}} \cr
         & = & \ex{X_{n - 1}} + 1 \cr
         & = & (n - 1) + 1 \cr
         & = & n, \cr
\end{array}
\]
where the second last equality follows by induction hypothesis.
%%
\item Since $\var{X_n} = \ex{X_n^2} - \exf^2\ofb{X_n}$ by (C.31) and $\ex{X_n} = n$ by part (a), it suffices to observe
\[
\exf^2\ofb{X_0} = 0
\]
and evaluate
\[
\begin{array}{lll}
\exf^2\ofb{X_n} & = & \ds{\sum_{s \in \cartpwr{\sete{0, 1}}{n}} (100i(s))^2 \multp \pr{A_s}} \cr
            & = & \ds{\sum_{s \in \cartpwr{\sete{0, 1}}{n - 1}} ((100i(s1))^2 \multp \pr{A_{s1}} + (100i(s0))^2 \multp \pr{A_{s0}})} \cr
            & = & \ex{X_{n - 1}^2} + 2 \ex{X_{n - 1}} + 100 \cr
\end{array}
\]
for $n \geq 1$. Alternatively, observe that for $n_i = 100i$ we have that
\[
\pr{X_n / 100 = k} = \bindist(k; n, 1/100)
\]
with the chance of success $1/100$. By Exercise C.3-10, it follows that
\[
\var{X_n} = 100^2 \var{X_n / 100}.
\]
Then use the fact that
\[
\var{X_n / 100} = n \times 1/100 \times (1 - 1/100)
\]
by (C.44). Either way, we get $\var{X_n} = 99n$.
%%
\end{enumerate}
%
\item
\begin{enumerate}[(a)]
%%
\item The procedure is given as follows.
\begin{codebox}
\Procname{$\proc{Random-Search}(A, x)$}
\li $n \gets \attrib{A}{length}$
\li create an empty array $E$ of length $n$
\li \For $i \gets 1$ \To $n$
\li \Do
        $E[i] \gets \const{false}$
    \End
\li $\id{num-elements-examined} \gets 0$
\li \Repeat
\li      $i \gets \proc{Random}(1, n)$
\li      \If $A[i] \isequal x$
\li          \Then
                   \Return $i$
\li          \Else
                   \If $E[i] \isequal \const{false}$
\li                    \Then
                             $E[i] \gets \const{true}$
\li                          $\id{num-elements-examined} \gets \id{num-elements-examined} + 1$
                       \End
             \End
\li \Until $\id{num-elements-examined} \isequal n$
\li print ``not found''
\end{codebox}
%%
\item If there is exactly one index $i$ such that $A[i] = x$, then
\[
\pr{\mathtext{the procedure \mathmode{\proc{Random}(1, n)} picks the index \mathmode{i} into \mathmode{A} such that \mathmode{A[i] = x}}} = \frac{1}{n}.
\]
Let $X$ be the number of indices into $A$ that must be picked before $x$ is found and $\proc{Random-Search}$ terminates. It follows that $\pr{X = r} = q^{r - 1}p$ is a geometric distribution, where $p = 1/n$ and $q = 1 - p$. Therefore, $\ex{X} = 1/p = n$.
%%
\item In this case, we have
\[
\pr{\mathtext{the procedure \mathmode{\proc{Random}(1, n)} picks the index \mathmode{i} into \mathmode{A} such that \mathmode{A[i] = x}}} = \frac{k}{n}.
\]
The probability $\pr{X = r} = q^{r - 1}p$ is a geometric distribution with $p = k/n$ and $q = 1 - p$. Thus, $\ex{X} = 1/p = n/k$.
%%
\item If there are no indices $i$ such that $A[i] = x$, then the case is the same as the third question in 5.4.2 (balls and bins) in textbook, i.e., the \emph{coupon collector's problem}. Let $X$ be the number of indices into $A$ that must be picked before all elements of $A$ have been checked and $\proc{Random-Search}$ terminates. Then, it follows that $\ex{X} = n(\ln n + \bigoh{1})$.
%%
\item Let $X$ be the index such that $A[X] = x$ (equivalently, the number of indices into $A$ that must be checked before $x$ is found). Then, we have $1 \leq X \leq n$ and, by the assumption that all possible permutations of the input array are equally likely, that
\[
\pr{X = i} = \frac{1}{n}.
\]
Hence,
\[
\ex{X} = \sum^n_{i = 1} i \multp \pr{X = i} = \frac{n + 1}{2}
\]
and the average-case running time of $\proc{Deterministic-Search}$ is $\bigtheta{\ex{X}} = \bigtheta{n}$. Its worst-case running time is $\bigtheta{n}$.
%%
\item Let $X$ be the index such that $A[X] = x$ and $A[j] \neq x$ for all $j < X$ (equivalently, the number of indices into $A$ that must be checked before $x$ is found). Then, we have $1 \leq X \leq n - k + 1$ and, by the assumption that all possible permutations of the input array are equally likely, that
\[
\pr{X = i} = \frac{{n - i \choose k - 1}}{\sum^{n - k + 1}_{j = 1} {n - j \choose k - 1}}.
\]
The average-case running time of $\proc{Deterministic-Search}$ in this case is $\bigtheta{\ex{X}}$. Its worst-case running time is $\bigtheta{n - k + 1} = \bigtheta{n - k}$.
%%
\item Both average-case and worst-case running times are $\bigtheta{n}$.
%%
\item Besides the time spent on randomly permuting the input array,\footnote{The procedure $\proc{Randomly-Permute}$ on page 136 of the textbook obviously takes $\bigtheta{n}$ time.} the worst-case and expected running times of $\proc{Scramble-Search}$ for the cases $k = 0$, $k = 1$ and $k \geq 1$ in general are the same as the average-case and worst-case running times of $\proc{Deterministic-Search}$ given in parts (g), (e) and (f), respectively.
%%
\item The procedure $\proc{Random-Search}$ exhibits the worst-case running time $\bigtheta{n \lg n}$, which is worse than the other two. The average-case and worst-case running times of $\proc{Deterministic-Search}$ are the same as the expected and worst-case running times of $\proc{Scramble-Search}$ (except for the latter's cost incurred by doing random permutation of the input array, which is negligible asymptotically), based on the assumption that \emph{all the possible permutations of the input array are equally likely}, i.e., the permutations conform to the \emph{uniform distribution}, whereas the procedure $\proc{Scramble-Search}$ enforces such a distribution by randomization so that no particular case of the input generates the worst-case behavior. Therefore, I would choose $\proc{Scramble-Search}$ except when some prior knowledge of the distribution of the input array is known to be in favor of $\proc{Deterministic-Search}$.
%%
\end{enumerate}
%
\end{enumerate}